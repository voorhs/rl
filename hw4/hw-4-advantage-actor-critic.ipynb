{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Advantage-Actor Critic (A2C) - 2 pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will implement Advantage Actor Critic algorithm that trains on a batch of Atari 2600 environments running in parallel. \n",
    "\n",
    "Firstly, we will use environment wrappers implemented in file `atari_wrappers.py`. These wrappers preprocess observations (resize, grayscal, take max between frames, skip frames, stack them together, prepares for PyTorch and normalizes to [0, 1]) and rewards. Some of the wrappers help to reset the environment and pass `done` flag equal to `True` when agent dies.\n",
    "File `env_batch.py` includes implementation of `ParallelEnvBatch` class that allows to run multiple environments in parallel. To create an environment we can use `nature_dqn_env` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install gym[accept-rom-license]==0.22.0\n",
    "# !pip install ale-py==0.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from atari_wrappers import nature_dqn_env\n",
    "\n",
    "nenvs = 8    # change this if you have more than 8 CPU ;)\n",
    "\n",
    "env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=nenvs, seed=0)\n",
    "\n",
    "n_actions = env.action_space.spaces[0].n\n",
    "obs = env.reset()\n",
    "assert obs.shape == (nenvs, 4, 84, 84)\n",
    "assert obs.dtype == np.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will need to implement a model that predicts logits of policy distribution and critic value. Use shared backbone. You may use same architecture as in DQN task with one modification: instead of having a single output layer, it must have two output layers taking as input the output of the last hidden layer (one for actor, one for critic). \n",
    "\n",
    "Still it may be very helpful to make more changes:\n",
    "* use orthogonal initialization with gain $\\sqrt{2}$ and initialize biases with zeros;\n",
    "* use more filters (e.g. 32-64-64 instead of 16-32-64);\n",
    "* use two-layer heads for actor and critic or add a linear layer into backbone;\n",
    "\n",
    "**Danger:** do not divide on 255, input is already normalized to [0, 1] in our wrappers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_size_out(size, kernel_size, stride):\n",
    "    \"\"\"\n",
    "    common use case:\n",
    "    cur_layer_img_w = conv2d_size_out(cur_layer_img_w, kernel_size, stride)\n",
    "    cur_layer_img_h = conv2d_size_out(cur_layer_img_h, kernel_size, stride)\n",
    "    to understand the shape for dense layer's input\n",
    "    \"\"\"\n",
    "    return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentNetwork(nn.Module):\n",
    "    def __init__(self, n_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_actions = n_actions\n",
    "\n",
    "        # Define your network body here. Please make sure agent is fully contained here\n",
    "        hparams = [\n",
    "            dict(in_channels=4, out_channels=32, kernel_size=3, stride=4),\n",
    "            dict(in_channels=32, out_channels=64, kernel_size=3, stride=2),\n",
    "            dict(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "        ]\n",
    "        self.conv = nn.Sequential(*[ConvBlock(**kwargs) for kwargs in hparams])\n",
    "        \n",
    "        width = 84\n",
    "        height = 84\n",
    "        for kwargs in hparams:\n",
    "            width = conv2d_size_out(width, kwargs['kernel_size'], kwargs['stride'])\n",
    "            height = conv2d_size_out(height, kwargs['kernel_size'], kwargs['stride'])\n",
    "        \n",
    "        dense_in_features = width * height * hparams[-1]['out_channels']\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Linear(in_features=dense_in_features, out_features=512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=512, out_features=n_actions)\n",
    "        )\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Linear(in_features=dense_in_features, out_features=512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=512, out_features=1, bias=False)\n",
    "        )\n",
    "        \n",
    "        self.apply(self.init)\n",
    "        \n",
    "\n",
    "    def forward(self, state_t):\n",
    "        \"\"\"\n",
    "        takes agent's observation (tensor), returns advantage and logits (tensor)\n",
    "        :param state_t: a batch of 4-frame buffers, shape = [batch_size, 4, h, w]\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.conv(state_t)\n",
    "        values = self.critic(x).squeeze(dim=-1)\n",
    "        logits = self.actor(x)\n",
    "\n",
    "        return values, logits\n",
    "    \n",
    "    def init(self, m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.orthogonal_(m.weight, np.sqrt(2))\n",
    "            nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Linear) and m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need to define and use a policy that wraps the model. While the model computes logits for all actions, the policy will sample actions and also compute their log probabilities.  `policy.act` should return a **dictionary** of all the arrays that are needed to interact with an environment and train the model.\n",
    "\n",
    "**Important**: \"actions\" will be sent to environment, they must be numpy array or list, not PyTorch tensor.\n",
    "\n",
    "Note: you can add more keys, e.g. it can be convenient to compute entropy right here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Policy:\n",
    "    def __init__(self, model: AgentNetwork):\n",
    "        self.model = model\n",
    "\n",
    "    def act(self, inputs):\n",
    "        '''\n",
    "        input:\n",
    "            inputs - numpy array, (batch_size x channels x width x height)\n",
    "        output: dict containing keys ['actions', 'logits', 'log_probs', 'values']:\n",
    "            'actions' - selected actions, numpy, (batch_size)\n",
    "            'logits' - actions logits, tensor, (batch_size x num_actions)\n",
    "            'log_probs' - log probs of selected actions, tensor, (batch_size)\n",
    "            'values' - critic estimations, tensor, (batch_size)\n",
    "        '''\n",
    "        device = next(self.model.parameters()).device\n",
    "        inputs = torch.from_numpy(inputs).to(device)\n",
    "        values, logits = self.model(inputs)\n",
    "\n",
    "        logprobs = F.log_softmax(logits, dim=1)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        entropy = probs.mul(logprobs).neg().sum(dim=1)\n",
    "\n",
    "        actions = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "        ids = torch.arange(len(values), device=values.device)\n",
    "        logprobs = logprobs[ids, actions]\n",
    "\n",
    "        return {\n",
    "            \"actions\": actions.detach().cpu().numpy(),\n",
    "            \"logits\": logits,\n",
    "            \"log_probs\": logprobs,\n",
    "            \"values\": values,\n",
    "            \"entropy\": entropy\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actions': array([5, 3, 5, 4, 4, 0, 0, 4]),\n",
       " 'logits': tensor([[-0.1037, -0.0054,  0.0194, -0.0670,  0.0138, -0.1633],\n",
       "         [-0.1906, -0.1316,  0.0053,  0.0645,  0.2488, -0.0135],\n",
       "         [-0.0799, -0.2727, -0.0153, -0.0436,  0.0280, -0.0983],\n",
       "         [-0.1188, -0.1467, -0.0109, -0.0309,  0.3081, -0.1928],\n",
       "         [-0.3331,  0.0728,  0.2201, -0.0444, -0.1000, -0.1762],\n",
       "         [-0.1401,  0.0501, -0.1286, -0.0133,  0.2314, -0.1620],\n",
       "         [ 0.0098, -0.0481, -0.1802, -0.0659,  0.0308, -0.0569],\n",
       "         [ 0.0236, -0.1886, -0.0806, -0.1430,  0.2154,  0.0183]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " 'log_probs': tensor([-1.9062, -1.7345, -1.8141, -1.4661, -1.8471, -1.9147, -1.7324, -1.5596],\n",
       "        grad_fn=<IndexBackward0>),\n",
       " 'values': tensor([-0.1297, -0.1377, -0.0250, -0.0449,  0.1353, -0.0191,  0.1572, -0.1123],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " 'entropy': tensor([1.7896, 1.7814, 1.7875, 1.7764, 1.7762, 1.7816, 1.7896, 1.7825],\n",
       "        grad_fn=<SumBackward1>)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(8, 4, 84, 84).numpy()\n",
    "agent = AgentNetwork(n_actions=n_actions)\n",
    "policy = Policy(agent)\n",
    "policy.act(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will pass the environment and policy to a runner that collects rollouts from the environment. \n",
    "The class is already implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from runners import EnvRunner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This runner interacts with the environment for a given number of steps and returns a dictionary containing\n",
    "keys \n",
    "\n",
    "* 'observations' \n",
    "* 'rewards' \n",
    "* 'dones'\n",
    "* 'actions'\n",
    "* all other keys that you defined in `Policy`\n",
    "\n",
    "under each of these keys there is a python `list` of interactions with the environment of specified length $T$ &mdash; the size of partial trajectory, or rollout length. Let's have a look at how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AgentNetwork(n_actions=n_actions)\n",
    "policy = Policy(model)\n",
    "runner = EnvRunner(env, policy, nsteps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generates new rollout\n",
    "trajectory = runner.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['actions', 'logits', 'log_probs', 'values', 'entropy', 'observations', 'rewards', 'dones'])\n"
     ]
    }
   ],
   "source": [
    "# what is inside\n",
    "print(trajectory.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "assert 'logits' in trajectory, \"Not found: policy didn't provide logits\"\n",
    "assert 'log_probs' in trajectory, \"Not found: policy didn't provide log_probs of selected actions\"\n",
    "assert 'values' in trajectory, \"Not found: policy didn't provide critic estimations\"\n",
    "assert trajectory['logits'][0].shape == (nenvs, n_actions), \"logits wrong shape\"\n",
    "assert trajectory['log_probs'][0].shape == (nenvs,), \"log_probs wrong shape\"\n",
    "assert trajectory['values'][0].shape == (nenvs,), \"values wrong shape\"\n",
    "\n",
    "for key in trajectory.keys():\n",
    "    assert len(trajectory[key]) == 5, \\\n",
    "    f\"something went wrong: 5 steps should have been done, got trajectory of length {len(trajectory[key])} for '{key}'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([-0.1643, -0.1643, -0.1643, -0.1643, -0.1643, -0.1643, -0.1643, -0.1643],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " tensor([-0.1643, -0.1643, -0.1643, -0.1643, -0.1643, -0.1643, -0.1643, -0.1643],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " tensor([-0.1643, -0.1643, -0.1643, -0.1643, -0.1643, -0.1643, -0.1643, -0.1643],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " tensor([-0.1588, -0.1588, -0.1588, -0.1588, -0.1588, -0.1588, -0.1588, -0.1588],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " tensor([-0.1680, -0.1680, -0.1680, -0.1680, -0.1680, -0.1680, -0.1680, -0.1680],\n",
       "        grad_fn=<SqueezeBackward1>)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectory['values']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's work with this trajectory a bit. To train the critic you will need to compute the value targets. It will also be used as an estimation of $Q$ for actor training.\n",
    "\n",
    "You should use all available rewards for value targets, so the formula for the value targets is simple:\n",
    "\n",
    "$$\n",
    "\\hat v(s_t) = \\sum_{t'=0}^{T - 1}\\gamma^{t'}r_{t+t'} + \\gamma^T \\hat{v}(s_{t+T}),\n",
    "$$\n",
    "\n",
    "where $s_{t + T}$ is the latest observation of the environment.\n",
    "\n",
    "Any callable could be passed to `EnvRunner` to be applied to each partial trajectory after it is collected. \n",
    "Thus, we can implement and use `ComputeValueTargets` callable. \n",
    "\n",
    "**Do not forget** to use `trajectory['dones']` flags to check if you need to add the value targets at the next step when \n",
    "computing value targets for the current step.\n",
    "\n",
    "**Bonus (+0.5 pts):** implement [Generalized Advantage Estimation (GAE)](https://arxiv.org/pdf/1506.02438.pdf) instead; use $\\lambda \\approx 0.95$ or even closer to 1 in experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectory['rewards']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([False, False, False, False, False, False, False, False]),\n",
       " array([False, False, False, False, False, False, False, False]),\n",
       " array([False, False, False, False, False, False, False, False]),\n",
       " array([False, False, False, False, False, False, False, False]),\n",
       " array([False, False, False, False, False, False, False, False])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectory['dones']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputeValueTargets:\n",
    "    def __init__(self, policy, gamma=0.99):\n",
    "        self.policy = policy\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def __call__(self, trajectory, latest_observation):\n",
    "        '''\n",
    "        This method should modify trajectory inplace by adding \n",
    "        an item with key 'value_targets' to it\n",
    "        \n",
    "        input:\n",
    "            trajectory - dict from runner\n",
    "            latest_observation - last state, numpy, (num_envs x channels x width x height)\n",
    "        '''\n",
    "        \n",
    "        rewards = trajectory['rewards'] # (env_steps, num_envs)\n",
    "        dones = trajectory['dones'] # (env_steps, num_envs)\n",
    "        env_steps = len(rewards)\n",
    "\n",
    "        value_estimate = policy.act(latest_observation)['values'].detach().cpu().numpy() # (num_envs,)\n",
    "\n",
    "        value_targets = []\n",
    "        for t in range(env_steps-1, -1, -1):\n",
    "            value_estimate = self.gamma * value_estimate * (1 - dones[t]) + rewards[t]\n",
    "            value_targets.append(value_estimate)\n",
    "\n",
    "        trajectory['value_targets'] = value_targets[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After computing value targets we will transform lists of interactions into tensors\n",
    "with the first dimension `batch_size` which is equal to `T * nenvs`.\n",
    "\n",
    "You need to make sure that after this transformation `\"log_probs\"`, `\"value_targets\"`, `\"values\"` are 1-dimensional PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions (8,)\n",
      "logits torch.Size([8, 6])\n",
      "log_probs torch.Size([8])\n",
      "values torch.Size([8])\n",
      "entropy torch.Size([8])\n",
      "observations (8, 4, 84, 84)\n",
      "rewards (8,)\n",
      "dones (8,)\n"
     ]
    }
   ],
   "source": [
    "for k, v in trajectory.items():\n",
    "    print(k, v[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeTimeBatch:\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "\n",
    "    \"\"\"Merges first two axes typically representing time and env batch.\"\"\"\n",
    "    def __call__(self, trajectory, latest_observation):\n",
    "        for k, v in trajectory.items():\n",
    "            if isinstance(v[0], np.ndarray):\n",
    "                tensor = torch.from_numpy(np.stack(v, axis=0).astype(np.float32)).to(self.device)\n",
    "                tensor = tensor.view(-1, *tensor.shape[2:])\n",
    "            else:\n",
    "                tensor = torch.concatenate(v, dim=0)\n",
    "            trajectory[k] = tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do more sanity checks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = EnvRunner(\n",
    "    env,\n",
    "    policy,\n",
    "    nsteps=5,\n",
    "    transforms=[\n",
    "        ComputeValueTargets(policy),\n",
    "        MergeTimeBatch(device='cuda')\n",
    "    ]\n",
    ")\n",
    "\n",
    "trajectory = runner.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More sanity checks\n",
    "assert 'value_targets' in trajectory, \"Value targets not found\"\n",
    "assert trajectory['log_probs'].shape == (5 * nenvs,)\n",
    "assert trajectory['value_targets'].shape == (5 * nenvs,)\n",
    "assert trajectory['values'].shape == (5 * nenvs,)\n",
    "\n",
    "assert trajectory['log_probs'].requires_grad, \"Gradients are not available for actor head!\"\n",
    "assert trajectory['values'].requires_grad, \"Gradients are not available for critic head!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions torch.Size([40])\n",
      "logits torch.Size([40, 6])\n",
      "log_probs torch.Size([40])\n",
      "values torch.Size([40])\n",
      "entropy torch.Size([40])\n",
      "observations torch.Size([40, 4, 84, 84])\n",
      "rewards torch.Size([40])\n",
      "dones torch.Size([40])\n",
      "value_targets torch.Size([40])\n"
     ]
    }
   ],
   "source": [
    "for k, v in trajectory.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is the time to implement the advantage actor critic algorithm itself. You can look into [Mnih et al. 2016](https://arxiv.org/abs/1602.01783) paper, and lectures ([part 1](https://www.youtube.com/watch?v=Ds1trXd6pos&list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A&index=5), [part 2](https://www.youtube.com/watch?v=EKqxumCuAAY&list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A&index=6)) by Sergey Levine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "class A2C:\n",
    "    def __init__(self, policy: Policy, optimizer: torch.optim.Optimizer, value_loss_coef=0.25, entropy_coef=0.01, max_grad_norm=0.5):\n",
    "        self.policy = policy\n",
    "        self.optimizer = optimizer\n",
    "        self.value_loss_coef = value_loss_coef\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "    \n",
    "    def loss(self, trajectory, write):\n",
    "        advantage = trajectory['value_targets'] - trajectory['values']\n",
    "        policy_loss = trajectory['log_probs'].mul(advantage.detach()).mean().neg()\n",
    "        entropy_loss = trajectory['entropy'].mean().neg()\n",
    "        critic_loss = F.mse_loss(trajectory['values'], trajectory['value_targets'].detach(), reduction='mean')\n",
    "\n",
    "        write(\n",
    "            'losses',\n",
    "            {\n",
    "                'policy loss': policy_loss,\n",
    "                'critic loss': critic_loss,\n",
    "                'entropy loss': entropy_loss\n",
    "            }\n",
    "        )\n",
    "\n",
    "        write('critic/advantage', advantage.mean())\n",
    "        write(\n",
    "            'critic/values',\n",
    "            {\n",
    "                'value predictions': trajectory['values'].mean(),\n",
    "                'value targets': trajectory['value_targets'].mean(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        write('Episodes/mode_action', torch.mode(trajectory['actions'])[0])\n",
    "        write('Episodes/actions', trajectory['actions'])\n",
    "\n",
    "        return policy_loss + self.value_loss_coef * critic_loss + self.entropy_coef * entropy_loss\n",
    "\n",
    "    def step(self, runner: EnvRunner):\n",
    "        trajectory = runner.get_next()\n",
    "        loss = self.loss(trajectory, runner.write)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        grad_norm = clip_grad_norm_(self.policy.model.parameters(), self.max_grad_norm)\n",
    "        self.optimizer.step()\n",
    "        runner.write('gradient norm', grad_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can train your model. For optimization we suggest you use RMSProp with learning rate 7e-4 (you can also linearly decay it to 0), smoothing constant (alpha in PyTorch) equal to 0.99 and epsilon equal to 1e-5.\n",
    "\n",
    "We recommend to train for at least 10 million environment steps across all batched environments (takes ~3 hours on a single GTX1080 with 8 CPU). It should be possible to achieve *average raw reward over last 100 episodes* (the average is taken over 100 last episodes in each environment in the batch) of about 600. **Your goal is to reach 500**.\n",
    "\n",
    "Notes:\n",
    "* if your reward is stuck at ~200 for more than 2M steps then probably there is a bug\n",
    "* if your gradient norm is >10 something probably went wrong\n",
    "* make sure your `entropy loss` is negative, your `critic loss` is positive\n",
    "* make sure you didn't forget `.detach` in losses where it's needed\n",
    "* `actor loss` should oscillate around zero or near it; do not expect loss to decrease in RL ;)\n",
    "* you can experiment with `nsteps` (\"rollout length\"); standard rollout length is 5 or 10. Note that this parameter influences how many algorithm iterations is required to train on 10M steps (or 40M frames --- we used frameskip in preprocessing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AgentNetwork(n_actions).cuda()\n",
    "policy = Policy(model)\n",
    "runner = EnvRunner(\n",
    "    env,\n",
    "    policy,\n",
    "    nsteps=10,\n",
    "    transforms=[\n",
    "        ComputeValueTargets(policy),\n",
    "        MergeTimeBatch(device='cuda')\n",
    "    ]\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=7e-4, alpha=0.99, eps=1e-5)\n",
    "\n",
    "a2c = A2C(policy, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentNetwork(\n",
       "  (conv): Sequential(\n",
       "    (0): ConvBlock(\n",
       "      (net): Sequential(\n",
       "        (0): Conv2d(4, 32, kernel_size=(3, 3), stride=(4, 4))\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ConvBlock(\n",
       "      (net): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (2): ConvBlock(\n",
       "      (net): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (actor): Sequential(\n",
       "    (0): Flatten()\n",
       "    (1): Linear(in_features=4096, out_features=512, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Linear(in_features=512, out_features=6, bias=True)\n",
       "  )\n",
       "  (critic): Sequential(\n",
       "    (0): Flatten()\n",
       "    (1): Linear(in_features=4096, out_features=512, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Linear(in_features=512, out_features=1, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('A2C-2.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm.autonotebook import trange\n",
    "\n",
    "\n",
    "# n_steps = int(1e7 / nenvs / 10)\n",
    "# for i_step in trange(n_steps):\n",
    "#     a2c.step(runner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save your model just in case \n",
    "# torch.save(model.state_dict(), \"A2C-2.pth\")\n",
    "# torch.save(optimizer.state_dict(), \"A2C-2-optim.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=None, \n",
    "                     clip_reward=False, summaries=False, episodic_life=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, policy, n_games=1, t_max=10000):\n",
    "    '''\n",
    "    Plays n_games and returns rewards\n",
    "    '''\n",
    "    rewards = []\n",
    "    \n",
    "    for _ in range(n_games):\n",
    "        s = env.reset()\n",
    "        \n",
    "        R = 0\n",
    "        for _ in range(t_max):\n",
    "            action = policy.act(np.array([s]))[\"actions\"][0]\n",
    "            \n",
    "            s, r, done, _ = env.step(action)\n",
    "            \n",
    "            R += r\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(R)\n",
    "    return np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your score: 648.0\n",
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "# evaluation will take some time!\n",
    "sessions = evaluate(env, policy, n_games=30)\n",
    "score = sessions.mean()\n",
    "print(f\"Your score: {score}\")\n",
    "\n",
    "assert score >= 500, \"Needs more training?\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env_monitor = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=None, monitor=True,\n",
    "                             clip_reward=False, summaries=False, episodic_life=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# record sessions\n",
    "sessions = evaluate(env_monitor, policy, n_games=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 740.,  570., 1320.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rewards for recorded games\n",
    "sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_monitor.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
